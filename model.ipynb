{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79274c7",
   "metadata": {},
   "source": [
    "# ReceiptIQ Model\n",
    "\n",
    "This is the model behind the receiptiq application. It takes a receipt/invoice image and a json schema and generates the desired output in json format with each JSON leaf containing both the extracted value and the bounding box coordinates in format [x,y,w,h]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572781ac",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The starting dataset is a composite of images only from:\n",
    "- [Receipt or Invoice Computer Vision Model](https://universe.roboflow.com/jakob-awn1e/receipt-or-invoice)\n",
    "- [OCR Receipts Text Detection - retail dataset](https://www.kaggle.com/datasets/trainingdatapro/ocr-receipts-text-detection)\n",
    "- [Receipt Dataset for information extraction](https://www.kaggle.com/datasets/dhiaznaidi/receiptdatasetssd300v2)\n",
    "\n",
    "Totaling `7,334 images` in `datasets/images` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed7a420",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Given this dataset does not natively contain extracted information and importantly the bounding boxes (Some contain data but it's incomplete and mostly doesn't have the bounding information), the raw images were sent through a larger model to distill the high quality training data from the receipts. In this case it is `gpt4o` from openai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49902be",
   "metadata": {},
   "source": [
    "### Model disitilation\n",
    "\n",
    "Below script was tested on an initial batch of 10 images in order to finetune the prompt and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-5-nano\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You're a document parser. Given an image of a receipt, \"\n",
    "    \"extract structured data in JSON including all leaf-node values and their bounding boxes (x,y,w,h). For the leaf nodes\" \\\n",
    "    \"use the format `name: {{value: actual value, bbox: bounding box in the format (x,y,w,h), descr: description of the field}} \"\n",
    ")\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def encode_image(file_path: str) -> str:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    return base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "def call_openai(image_b64: str) -> dict:\n",
    "    try:\n",
    "        payload = {\n",
    "            \"model\": MODEL,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_b64}\"}},\n",
    "                        {\"type\": \"text\", \"text\": \"Please extract the structured JSON for this receipt.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        response = client.chat.completions.create(\n",
    "                    **payload,\n",
    "                    temperature=0.1,  # Low temperature for consistent extraction\n",
    "                    max_tokens=2000\n",
    "                )\n",
    "        result_text = response.choices[0].message.content.strip().replace(\"```json\\n\",\"\").replace(\"```\",\"\")\n",
    "        return result_text\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "def get_image_files(input_dir: str):\n",
    "    return glob.glob(os.path.join(input_dir, \"*.jpg\")) + glob.glob(os.path.join(input_dir, \"*.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3566421",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"datasets/images\"\n",
    "output_dir = \"datasets/data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "file_list = get_image_files(input_dir)[:10]\n",
    "print(f\"Found {len(file_list)} images. Starting...\")\n",
    "\n",
    "start = time.time()\n",
    "for file in tqdm(file_list):\n",
    "    if os.path.exists(f\"{output_dir}/{os.path.basename(file)}.json\"):\n",
    "        continue\n",
    "    image_enconded = encode_image(file)\n",
    "    receipt_data = call_openai(image_enconded)\n",
    "    with open(f\"{output_dir}/{os.path.basename(file)}.json\",\"w\") as f:\n",
    "        f.write(receipt_data)\n",
    "print(f\"Completed in {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dfaf39",
   "metadata": {},
   "source": [
    "Sample json file\n",
    "```json\n",
    "{\n",
    "  \"store_info\": {\n",
    "    \"name\": {\n",
    "      \"value\": \"POP TATES R-MALL\",\n",
    "      \"bbox\": [50, 20, 150, 20],\n",
    "      \"descr\": \"Store name\"\n",
    "    },\n",
    "    \"address\": {\n",
    "      \"value\": \"MULUND WEST, MUMBAI - 421 004\",\n",
    "      \"bbox\": [50, 40, 200, 20],\n",
    "      \"descr\": \"Store address\"\n",
    "    },\n",
    "    \"phone\": {\n",
    "      \"value\": \"TEL: 2591 2591\",\n",
    "      \"bbox\": [50, 60, 150, 20],\n",
    "      \"descr\": \"Store phone number\"\n",
    "    }\n",
    "  },\n",
    "  \"transaction_info\": {\n",
    "    \"bill_no\": {\n",
    "      \"value\": \"2708/020612/3V\",\n",
    "      \"bbox\": [50, 100, 150, 20],\n",
    "      \"descr\": \"Bill number\"\n",
    "    }\n",
    "    ...\n",
    "```\n",
    "\n",
    "Having tested on 10 images which took 204s i.e 20s/image:\n",
    "the collection of 7334 images would take `7,334 images * 20 s/image = 146,680 s approx 40hr 45 minutes` which is inefficient.\n",
    "Coupled with the cheaper pricing for batch inference and api rate limits, batch inference is prefered for model distillation.\n",
    "Nonethless, a batch of 500 was used to refine the the rest of the prep and training while full batch was being prepared\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e8919",
   "metadata": {},
   "source": [
    "### Batch model distilation\n",
    "\n",
    "Use openai's batch api to send batches of prompts in jsonl format (max accepted size is 200MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df08790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import shutil\n",
    "from typing import List\n",
    "\n",
    "MAX_ITEMS_PER_BATCH = 500\n",
    "OUTPUT_DIR = \"datasets/batch_files\"\n",
    "BASE_URL = \"https://receiptiq-model-finetuning-receipts.t3.storageapi.dev\"\n",
    "COMPLETION_WINDOW = \"24h\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def encode_image(path: str) -> str:\n",
    "    \"\"\"Read image as-is and encode to base64.\"\"\"\n",
    "    with open(path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def prepare_batch_files(image_files_list: List[str], output_dir: str):\n",
    "    shutil.rmtree(output_dir)\n",
    "    batch_files = []\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    batch_count = 0\n",
    "    current_batch = []\n",
    "\n",
    "    for img_file in tqdm(image_files_list, desc=\"Preparing batch files\"):\n",
    "        file_name = img_file.split(\"/\")[-1]\n",
    "        img_url = f\"{BASE_URL}/{file_name}\"\n",
    "        payload = {\n",
    "            \"custom_id\": img_file,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\"url\": img_url}\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": \"Please extract the structured JSON for this receipt. Respond only in json format of your best approximation.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        current_batch.append(payload)\n",
    "\n",
    "        if len(current_batch) >= MAX_ITEMS_PER_BATCH:\n",
    "            batch_path = os.path.join(output_dir, f\"batch_{batch_count}.jsonl\")\n",
    "            with open(batch_path, \"w\") as f:\n",
    "                for item in current_batch:\n",
    "                    f.write(json.dumps(item) + \"\\n\")\n",
    "            batch_count += 1\n",
    "            current_batch = []\n",
    "            batch_files.append(batch_path)\n",
    "\n",
    "    # Write remaining items\n",
    "    if current_batch:\n",
    "        batch_path = os.path.join(output_dir, f\"batch_{batch_count}.jsonl\")\n",
    "        batch_files.append(batch_path)\n",
    "        with open(batch_path, \"w\") as f:\n",
    "            for item in current_batch:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    print(f\"[✓] Created {batch_count + (1 if current_batch else 0)} batch files in {output_dir}\")\n",
    "    return batch_files\n",
    "\n",
    "POLL_INTERVAL = 60\n",
    "\n",
    "def wait_for_completion(batch_id: str):\n",
    "    \"\"\"Poll until batch is completed or failed\"\"\"\n",
    "    while True:\n",
    "        resp = client.batches.retrieve(batch_id)\n",
    "        status = resp.status\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Batch {batch_id} status: {status}\")\n",
    "        if status in (\"completed\", \"failed\", \"expired\", \"cancelled\"):\n",
    "            return status\n",
    "        time.sleep(POLL_INTERVAL)\n",
    "\n",
    "def upload_batch_file(filepath: str):\n",
    "    \"\"\"Upload a JSONL file for batch processing\"\"\"\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        resp = client.files.create(file=f, purpose=\"batch\")\n",
    "    return resp.id\n",
    "\n",
    "def start_batch(file_id: str, batch_name: str):\n",
    "    \"\"\"Start a batch job from an uploaded file\"\"\"\n",
    "    resp = client.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=COMPLETION_WINDOW,\n",
    "        metadata={\"name\": batch_name}\n",
    "    )\n",
    "    return resp.id\n",
    "\n",
    "PROCESSED_FILE = \"processed_images.json\"\n",
    "\n",
    "def load_processed():\n",
    "    if os.path.exists(PROCESSED_FILE):\n",
    "        with open(PROCESSED_FILE) as f:\n",
    "            return set(json.load(f))\n",
    "    return set()\n",
    "\n",
    "def save_processed(processed_set):\n",
    "    with open(PROCESSED_FILE, \"w\") as f:\n",
    "        json.dump(list(processed_set), f)\n",
    "\n",
    "input_dir = \"datasets/images\"\n",
    "output_dir = \"datasets/batch_files\"\n",
    "INPUT_DIR = \"datasets/images\"\n",
    "BATCH_NAME_PREFIX = \"receiptiq_model_batch\"\n",
    "images_files_list = get_image_files(INPUT_DIR)\n",
    "processed_images = load_processed()\n",
    "remaining_images = [img for img in images_files_list if img not in processed_images]\n",
    "batch_files = prepare_batch_files(remaining_images, output_dir=output_dir)\n",
    "\n",
    "# Upload & create batches\n",
    "for idx, batch_file in enumerate(batch_files, start=1):\n",
    "    print(f\"\\n=== Processing batch {idx}/{len(batch_files)}: {batch_file} ===\")\n",
    "    file_id = upload_batch_file(batch_file)\n",
    "    print(f\"[✓] Uploaded file {batch_file} → File ID: {file_id}\")\n",
    "\n",
    "    batch_name = f\"{BATCH_NAME_PREFIX}{idx}\"\n",
    "    batch_id = start_batch(file_id, batch_name)\n",
    "    print(f\"[✓] Started batch {batch_name} → Batch ID: {batch_id}\")\n",
    "\n",
    "    status = wait_for_completion(batch_id)\n",
    "    print(f\"[!] Batch {batch_name} finished with status: {status}\")\n",
    "\n",
    "    if status == \"completed\":\n",
    "        with open(batch_file) as bf:\n",
    "            for line in bf:\n",
    "                data = json.loads(line)\n",
    "                processed_images.add(data[\"custom_id\"])\n",
    "        save_processed(processed_images)\n",
    "        time.sleep(300)\n",
    "    else:\n",
    "        print(f\"[!] Batch failed: {batch_name}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6edeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_and_data = []\n",
    "faulty_extractions = []\n",
    "for batch in client.batches.list():\n",
    "    if batch.status == \"completed\":\n",
    "        try:\n",
    "            output = client.files.content(batch.output_file_id)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        for receipt in output.text.splitlines():\n",
    "            completion_response_json = json.loads(receipt)\n",
    "            receipt_dict = dict(json.loads(receipt)).get(\"response\").get(\"body\").get(\"choices\")[0]\n",
    "            receipt_data = receipt_dict.get(\"message\").get(\"content\")\n",
    "            try:\n",
    "                parsed_data = json.loads(receipt_data)\n",
    "                with open(f'datasets/data/{completion_response_json.get(\"custom_id\").split(\"/\")[-1]}.json',\"w\") as df:\n",
    "                    df.write(receipt_data)\n",
    "                receipts_and_data.append({\n",
    "                    \"image_filename\": f'{completion_response_json.get(\"custom_id\").split(\"/\")[-1]}',\n",
    "                    \"data\": parsed_data\n",
    "                })\n",
    "            except:\n",
    "                faulty_extractions.append(completion_response_json.get(\"custom_id\").split(\"/\")[-1])\n",
    "print(f\"{len(receipts_and_data)} receipts\")\n",
    "print(f\"{len(faulty_extractions)} failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae371872",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import asdict\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "from datasets import Dataset, Features, Value, Image as HFImage\n",
    "\n",
    "hf_model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(hf_model_id)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "def get_schema(data: dict):\n",
    "    schema = {}\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, dict):\n",
    "            if \"value\" in v:\n",
    "                schema[k] = f\"string//{v.get('descr', '')}\"\n",
    "            else:\n",
    "                schema[k] = get_schema(v)\n",
    "        elif isinstance(v, list):\n",
    "            schema[k] = [get_schema(v[0]) if len(v)>0 else {}]\n",
    "    return schema\n",
    "\n",
    "# remove those whose data does not match expected schema\n",
    "receipts_and_data_schema_checked = []\n",
    "for item in tqdm(receipts_and_data):\n",
    "    try:\n",
    "        schema = get_schema(item['data'])\n",
    "        receipts_and_data_schema_checked.append(item)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        # print(item[\"image_filename\"])\n",
    "        # raise\n",
    "        pass\n",
    "for item in receipts_and_data_schema_checked:\n",
    "    item[\"data\"] = json.dumps(item[\"data\"], ensure_ascii=False)\n",
    "    item['image_filename'] = f\"datasets/images/{item['image_filename']}\"\n",
    "\n",
    "print(receipts_and_data_schema_checked[0])\n",
    "features = Features({\n",
    "    \"image_filename\": Value(\"string\"),\n",
    "    \"data\": Value(\"string\")  # we'll store as JSON string for Arrow compatibility\n",
    "})\n",
    "receipts_and_data_dataset = Dataset.from_list(receipts_and_data_schema_checked, features=features)\n",
    "receipts_and_data_dataset = receipts_and_data_dataset.cast_column(\"image_filename\", HFImage(decode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ef24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    import json\n",
    "    from PIL import Image as PILImage\n",
    "\n",
    "    # Convert JSON string back to dict for schema extraction\n",
    "    data_dict = json.loads(batch[\"data\"])\n",
    "    schema_str = json.dumps(get_schema(data_dict), ensure_ascii=False)\n",
    "\n",
    "    # HF Image column is already decoded as PIL.Image\n",
    "    image = batch[\"image_filename\"]\n",
    "\n",
    "    # Tokenize input\n",
    "    input_tokens = processor(\n",
    "        image, f\"<|image|><|begin_of_text|>{schema_str}\", return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize output\n",
    "    output_tokens = tokenizer(json.dumps(data_dict, ensure_ascii=False), return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_tokens[\"input_ids\"][0],   # remove batch dim\n",
    "        \"attention_mask\": input_tokens[\"attention_mask\"][0],\n",
    "        \"output_ids\": output_tokens[\"input_ids\"][0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21798bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_and_data_dataset =receipts_and_data_dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_and_data_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a7415",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from typing import Dict\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from utils import load_model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beea84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "base_model_path: str = \"models/llama/Llama-3.2-11B-Vision-base\"\n",
    "\n",
    "finetune_dataset_path = \"dataset\"\n",
    "\n",
    "finetuned_model_path = \"models/llama/Llama-3.2-11B-Vision-ReceiptIQ-tuned\"\n",
    "if not os.path.exists(finetuned_model_path):\n",
    "    os.makedirs(finetuned_model_path)\n",
    "\n",
    "model, processor = load_model_quantized(base_model_path, hf_model_id)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "finetuned_model = get_peft_model(model, lora_config)\n",
    "finetuned_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1faae8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Prepare finetuning dataset of receipts\n",
    "It the dataset comes from kaggle `https://www.kaggle.com/datasets/dhiaznaidi/receiptdatasetssd300v2` and contains\n",
    "- Images under `dataset/images` of receipts\n",
    "- Extracted data under `dataset/gdt` containing `company`, `total`, `date` and `address`\n",
    "- Data about extracted data in `data/info_data` contains the data with important words and their coordinates\n",
    "\n",
    "The expected data should contain `receipt_path`, `schema`(for now just the company, date, total and address), and the `output`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98647754",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ReceiptData:\n",
    "    receipt_path: str\n",
    "    schema: Dict\n",
    "    output: Dict\n",
    "\n",
    "fixed_schema = {\n",
    "    \"total\": \"number//total amount of the invoice\",\n",
    "    \"company\": \"string//the name of the company or person doing the supply\",\n",
    "    \"date\": \"date//the date of the invoice\",\n",
    "    \"address\": \"string//address of the person or company doing the supply\",\n",
    "}\n",
    "\n",
    "def prepare_dataset() -> List[ReceiptData]:\n",
    "    dataset: List[ReceiptData] = []\n",
    "    for receipt in os.listdir(f\"{finetune_dataset_path}/info_data\"):\n",
    "        with open(f\"{finetune_dataset_path}/info_data/{receipt}\", \"r\") as f:\n",
    "            receipt_info = json.loads(f.read())\n",
    "            receipt_id  = receipt_info.get(\"image_path\",\"\").split(\"/\")[5].replace(\".jpg\",\"\")\n",
    "            with open(f\"{finetune_dataset_path}/gdt/{receipt_id}.json\",\"r\") as df:\n",
    "                extracted_data = json.loads(df.read())\n",
    "                for k,v in extracted_data.items():\n",
    "                    if k in receipt_info.keys():\n",
    "                        extracted_data[k] = {\n",
    "                            \"value\": v,\n",
    "                            \"coordinates\": receipt_info[k]\n",
    "                        }\n",
    "                    else:\n",
    "                        extracted_data[k] = {\n",
    "                            \"value\": v,\n",
    "                            \"coordinates\": {\n",
    "                                \"xmin\": 0,\n",
    "                                \"ymin\": 0,\n",
    "                                \"xmax\": 0,\n",
    "                                \"ymax\": 0\n",
    "                            }\n",
    "                        }\n",
    "            receipt_data = ReceiptData(\n",
    "                receipt_path=receipt_info.get(\"image_path\",\"\").replace(\"/content/Dataset/train/\",f\"{finetune_dataset_path}/\"),\n",
    "                schema=fixed_schema,\n",
    "                output=extracted_data\n",
    "            )\n",
    "            dataset.append(receipt_data)\n",
    "    return dataset\n",
    "data_list = prepare_dataset()\n",
    "print(len(data_list))\n",
    "pprint.pprint(data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875b6ebd",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d3e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from PIL import Image\n",
    "from dataclasses import asdict\n",
    "from datasets import Dataset\n",
    "\n",
    "def convert_to_dataset(data_list: List[ReceiptData]) -> Dataset:\n",
    "    data_dict = {\n",
    "        \"prompt\": [asdict(record) for record in data_list], \n",
    "        \"completion\": [{\"output\": record.output} for record in data_list]\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "    return dataset\n",
    "\n",
    "dataset = convert_to_dataset(data_list=data_list)\n",
    "\n",
    "def tokenize_batched(examples: Dict[str, List]) -> Dict[str, torch.Tensor]:\n",
    "    receipt_paths = [item.get(\"receipt_path\") for item in examples[\"prompt\"]]\n",
    "    schemas = [item.get(\"schema\") for item in examples[\"prompt\"]]\n",
    "    outputs = [f'{item.get(\"output\")}' for item in examples[\"completion\"]]\n",
    "    \n",
    "    images = [[Image.open(path)] for path in receipt_paths] \n",
    "    input_prompts = [f'<|image|><|begin_of_text|>{schema}' for schema in schemas]\n",
    "    \n",
    "    model_inputs = processor(\n",
    "        images=images,\n",
    "        text=input_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True\n",
    "    )\n",
    "      \n",
    "    completion_tokens = tokenizer(\n",
    "        outputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    batch_input_ids = []\n",
    "    batch_attention_mask = []\n",
    "    batch_labels = []\n",
    "\n",
    "    # Determine max length across all combined sequences in this batch\n",
    "    max_len = 0\n",
    "    for i in range(len(input_prompts)):\n",
    "        combined_len = model_inputs[\"input_ids\"][i].shape[0] + completion_tokens[\"input_ids\"][i].shape[0]\n",
    "        if combined_len > max_len:\n",
    "            max_len = combined_len\n",
    "    \n",
    "    # Optional: Cap max_len to a global maximum for consistency\n",
    "    # if max_len > 1024: max_len = 1024 \n",
    "\n",
    "    for i in range(len(input_prompts)):\n",
    "        prompt_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        prompt_attention_mask = model_inputs[\"attention_mask\"][i]\n",
    "        \n",
    "        output_input_ids = completion_tokens[\"input_ids\"][i]\n",
    "        output_attention_mask = completion_tokens[\"attention_mask\"][i]\n",
    "\n",
    "        combined_input_ids = torch.cat((prompt_input_ids, output_input_ids))\n",
    "        combined_attention_mask = torch.cat((prompt_attention_mask, output_attention_mask))\n",
    "\n",
    "        labels = combined_input_ids.clone()\n",
    "        prompt_length = prompt_input_ids.shape[0]\n",
    "        labels[:prompt_length] = -100\n",
    "\n",
    "        # Pad to max_len of the batch\n",
    "        current_len = combined_input_ids.shape[0]\n",
    "        if current_len < max_len:\n",
    "            padding_len = max_len - current_len\n",
    "            combined_input_ids = torch.cat([combined_input_ids, torch.full((padding_len,), tokenizer.pad_token_id, dtype=torch.long)])\n",
    "            combined_attention_mask = torch.cat([combined_attention_mask, torch.zeros(padding_len, dtype=torch.long)])\n",
    "            labels = torch.cat([labels, torch.full((padding_len,), -100, dtype=torch.long)])\n",
    "\n",
    "        batch_input_ids.append(combined_input_ids)\n",
    "        batch_attention_mask.append(combined_attention_mask)\n",
    "        batch_labels.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(batch_input_ids),\n",
    "        \"attention_mask\": torch.stack(batch_attention_mask),\n",
    "        \"labels\": torch.stack(batch_labels),\n",
    "        \"pixel_values\": model_inputs[\"pixel_values\"],\n",
    "        \"aspect_ratio_ids\": model_inputs[\"aspect_ratio_ids\"],\n",
    "        \"aspect_ratio_mask\": model_inputs['aspect_ratio_mask'],\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_batched,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b46521",
   "metadata": {},
   "source": [
    "# LoRA Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 1e-4\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "\n",
    "optimizer = torch.optim.AdamW(finetuned_model.parameters(), lr=lr)\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True if device == \"cuda\" else False,\n",
    "    num_workers=2\n",
    ")\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "\n",
    "print(f\"Training on device: {device}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\" if device == \"cuda\" else \"Using CPU\")\n",
    "config = {\n",
    "    \"lr\": lr,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"device\": device,\n",
    "    \"lr_scheduler\": \"linear_schedule_with_warmup\",\n",
    "    \"optimizer\": \"AdamW\"\n",
    "}\n",
    "with wandb.init(config=config) as run:\n",
    "    run.watch(finetuned_model)\n",
    "    finetuned_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = finetuned_model(**batch)\n",
    "            print(f\"forward pass done\")\n",
    "            loss = outputs.loss\n",
    "            print(f\"loss: {loss}\")\n",
    "            loss.backward()\n",
    "            print(f\"back pass done\")\n",
    "            optimizer.step()\n",
    "            print(f\"optimizer step done\")\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"{step=}: {loss=}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808cc90a",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_now = time.time()\n",
    "finetuned_model.save_pretrained(os.path.join(finetuned_model_path, f\"receiptiq_model_{time_now}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
